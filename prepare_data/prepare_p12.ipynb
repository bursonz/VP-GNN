{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Parse outcomes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "df_outcomes_a = pd.read_csv('../datasets/p12/rawdata/Outcomes-a.txt', sep=\",\", header=0,\n",
    "                            names=[\"RecordID\", \"SAPS-I\", \"SOFA\", \"Length_of_stay\", \"Survival\", \"In-hospital_death\"])\n",
    "df_outcomes_b = pd.read_csv('../datasets/p12/rawdata/Outcomes-b.txt', sep=\",\", header=0,\n",
    "                            names=[\"RecordID\", \"SAPS-I\", \"SOFA\", \"Length_of_stay\", \"Survival\", \"In-hospital_death\"])\n",
    "df_outcomes_c = pd.read_csv('../datasets/p12/rawdata/Outcomes-c.txt', sep=\",\", header=0,\n",
    "                            names=[\"RecordID\", \"SAPS-I\", \"SOFA\", \"Length_of_stay\", \"Survival\", \"In-hospital_death\"])\n",
    "\n",
    "print(df_outcomes_a.head(n=5))\n",
    "print(df_outcomes_b.head(n=5))\n",
    "print(df_outcomes_c.head(n=5))\n",
    "\n",
    "arr_outcomes_a = np.array(df_outcomes_a)\n",
    "arr_outcomes_b = np.array(df_outcomes_b)\n",
    "arr_outcomes_c = np.array(df_outcomes_c)\n",
    "\n",
    "n_a = arr_outcomes_a.shape[0]\n",
    "n_b = arr_outcomes_b.shape[0]\n",
    "n_c = arr_outcomes_c.shape[0]\n",
    "print('n_a = %d, n_b = %d, n_c = %d' % (n_a, n_b, n_c))\n",
    "\n",
    "# merge dataframes\n",
    "arr_outcomes = np.concatenate([arr_outcomes_a, arr_outcomes_b, arr_outcomes_c], axis=0)\n",
    "n = arr_outcomes.shape[0]\n",
    "print(arr_outcomes.shape)\n",
    "\n",
    "y_inhospdeath = arr_outcomes[:, -1]\n",
    "print(\"Percentage of in-hosp death: %.2f%%\" % (np.sum(y_inhospdeath)/n*100))\n",
    "print(y_inhospdeath.shape)\n",
    "\n",
    "os.makedirs('../datasets/p12/processed_data', exist_ok=True)\n",
    "\n",
    "# Store outcomes in npy format\n",
    "np.save('../datasets/p12/processed_data/arr_outcomes.npy', arr_outcomes)\n",
    "print('arr_outcomes.npy saved')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. extract all parameters encountered across all patients"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def extract_unq_params(path):\n",
    "    params_all = set()\n",
    "    for fname in os.listdir(path):\n",
    "        if fname.endswith('.txt'):\n",
    "            df_temp = pd.read_csv(path + '/' + fname, sep=\",\", header=1, names=[\"time\", \"param\", \"value\"])\n",
    "            arr_data_temp = df_temp.values\n",
    "            params_temp = arr_data_temp[:, 1]\n",
    "            params_all.update(params_temp.tolist())\n",
    "\n",
    "    params_all = [p for p in params_all if str(p) != 'nan']\n",
    "    return params_all\n",
    "\n",
    "\n",
    "params_a = extract_unq_params('../datasets/p12/rawdata/set-a/')\n",
    "params_b = extract_unq_params('../datasets/p12/rawdata/set-b/')\n",
    "params_c = extract_unq_params('../datasets/p12/rawdata/set-c/')\n",
    "\n",
    "params = params_a + params_b + params_c\n",
    "param_list = list(set(params))\n",
    "print('#params:', len(param_list))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. remove 5 fields"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "param_list.remove(\"Gender\")\n",
    "param_list.remove(\"Height\")\n",
    "param_list.remove(\"Weight\")\n",
    "param_list.remove(\"Age\")\n",
    "param_list.remove(\"ICUType\")\n",
    "\n",
    "print(\"Parameters: \", param_list)\n",
    "print(\"Number of total parameters:\", len(param_list))\n",
    "\n",
    "# save variable names\n",
    "np.save('../datasets/p12/processed_data/ts_params.npy', param_list)\n",
    "print('ts_params.npy: the names of 36 variables')\n",
    "\n",
    "static_param_list = ['Age', 'Gender', 'Height', 'ICUType', 'Weight']\n",
    "np.save('../datasets/p12/processed_data/static_params.npy', static_param_list)\n",
    "print('save names of static descriptors: static_params.npy')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. parse variables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "def parse_all(path):\n",
    "    P_list = []\n",
    "    cnt = 0\n",
    "    allfiles = os.listdir(path)\n",
    "    allfiles.sort()\n",
    "    for fname in allfiles:\n",
    "        if not fname.endswith('.txt'):\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(path + '/' + fname, sep=\",\", header=1, names=[\"time\", \"param\", \"value\"])\n",
    "        df_demogr = df.iloc[0:5]\n",
    "        df_data = df.iloc[5:]\n",
    "\n",
    "        arr_demogr = df_demogr.values\n",
    "        arr_data = df_data.values\n",
    "\n",
    "        my_dict = {'id': int(fname.split('.')[0])}\n",
    "        my_dict['static'] = (arr_demogr[0, 2], arr_demogr[1, 2],\n",
    "                             arr_demogr[2, 2], arr_demogr[3, 2], arr_demogr[4, 2])\n",
    "\n",
    "        # time-series\n",
    "        n_pts = arr_data.shape[0]\n",
    "        ts_list = []\n",
    "        for i in range(n_pts):  # for each line\n",
    "            param = arr_data[i, 1]  # the name of variables\n",
    "            if param in param_list:\n",
    "                ts = arr_data[i, 0]  # time stamp\n",
    "                hrs, mins = float(ts[0:2]), float(ts[3:5])\n",
    "                value = arr_data[i, 2]  # value of variable\n",
    "                totalmins = 60.0*hrs + mins\n",
    "                ts_list.append((hrs, mins, totalmins, param, value))\n",
    "        my_dict['ts'] = ts_list\n",
    "\n",
    "        # append patient dictionary in master dictionary\n",
    "        P_list.append(my_dict)\n",
    "        cnt += 1\n",
    "    return P_list\n",
    "\n",
    "p_list_a = parse_all('../datasets/p12/rawdata/set-a/')\n",
    "p_list_b = parse_all('../datasets/p12/rawdata/set-b/')\n",
    "p_list_c = parse_all('../datasets/p12/rawdata/set-c/')\n",
    "P_list = p_list_a + p_list_b + p_list_c\n",
    "print('Length of P_list', len(P_list))\n",
    "\n",
    "np.save('../datasets/p12/processed_data/P_list.npy', P_list)\n",
    "print('P_list.npy saved')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "print('number of samples: ', len(P_list))\n",
    "print(len(param_list), param_list)\n",
    "print(len(static_param_list), static_param_list)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max unique time series length"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "n = len(P_list)\n",
    "max_tmins = 48*60\n",
    "len_ts = []\n",
    "\n",
    "for ind in range(n):  # for each patient\n",
    "    ts = P_list[ind]['ts']\n",
    "    unq_tmins = []\n",
    "    for sample in ts:  # for each instance (time point)\n",
    "        current_tmin = sample[2]\n",
    "        if (current_tmin not in unq_tmins) and (current_tmin < max_tmins):\n",
    "            unq_tmins.append(current_tmin)\n",
    "\n",
    "    len_ts.append(len(unq_tmins))\n",
    "print('max unique time series length:', np.max(len_ts))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Group all patient time series into arrays"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "extended_static_list = ['Age', 'Gender=0', 'Gender=1', 'Height', 'ICUType=1', 'ICUType=2', 'ICUType=3', 'ICUType=4', 'Weight']\n",
    "np.save('../datasets/p12/processed_data/extended_static_params.npy', extended_static_list)\n",
    "\n",
    "P_list = np.load('../datasets/p12/processed_data/P_list.npy', allow_pickle=True)\n",
    "arr_outcomes = np.load('../datasets/p12/processed_data/arr_outcomes.npy', allow_pickle=True)\n",
    "\n",
    "ts_params = np.load('../datasets/p12/processed_data/ts_params.npy', allow_pickle=True)\n",
    "static_params = np.load('../datasets/p12/processed_data/static_params.npy', allow_pickle=True)\n",
    "\n",
    "max_tmins = 48*60\n",
    "len_ts = []\n",
    "n = len(P_list)\n",
    "max_len = 215\n",
    "F = len(ts_params)\n",
    "\n",
    "PTdict_list = []\n",
    "max_hr = 0\n",
    "for ind in range(n):\n",
    "    ID = P_list[ind]['id']\n",
    "    static = P_list[ind]['static']\n",
    "    ts = P_list[ind]['ts']\n",
    "\n",
    "    # find unique times\n",
    "    unq_tmins = []\n",
    "    for sample in ts:\n",
    "        current_tmin = sample[2]\n",
    "        if (current_tmin not in unq_tmins) and (current_tmin < max_tmins):\n",
    "            unq_tmins.append(current_tmin)\n",
    "    unq_tmins = np.array(unq_tmins)\n",
    "\n",
    "    # one-hot encoding of categorical static variables\n",
    "    extended_static = [static[0], 0, 0, static[2], 0, 0, 0, 0, static[4]]\n",
    "    if static[1] == 0:\n",
    "        extended_static[1] = 1\n",
    "    elif static[1] == 1:\n",
    "        extended_static[2] = 1\n",
    "    if static[3] == 1:\n",
    "        extended_static[4] = 1\n",
    "    elif static[3] == 2:\n",
    "        extended_static[5] = 1\n",
    "    elif static[3] == 3:\n",
    "        extended_static[6] = 1\n",
    "    elif static[3] == 4:\n",
    "        extended_static[7] = 1\n",
    "\n",
    "    # construct array of maximal size\n",
    "    Parr = np.zeros((max_len, F))\n",
    "    Tarr = np.zeros((max_len, 1))\n",
    "\n",
    "    # for each time measurement find index and store\n",
    "    for sample in ts:\n",
    "        tmins = sample[2]\n",
    "        param = sample[-2]\n",
    "        value = sample[-1]\n",
    "        if tmins < max_tmins:\n",
    "            time_id = np.where(tmins == unq_tmins)[0][0]\n",
    "            param_id = np.where(ts_params == param)[0][0]\n",
    "            Parr[time_id, param_id] = value\n",
    "            Tarr[time_id, 0] = unq_tmins[time_id]\n",
    "\n",
    "    length = len(unq_tmins)\n",
    "\n",
    "    # construct dictionary\n",
    "    my_dict = {'id': ID, 'static': static, 'extended_static': extended_static, 'arr': Parr, 'time': Tarr, 'length': length}\n",
    "\n",
    "    # add array into list\n",
    "    PTdict_list.append(my_dict)\n",
    "\n",
    "print(len(PTdict_list))\n",
    "np.save('../datasets/p12/processed_data/PTdict_list.npy', PTdict_list)\n",
    "print('PTdict_list.npy saved', PTdict_list[0].keys())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\"\"\"Remove 12 patients at blacklist\"\"\"\n",
    "PTdict_list = np.load('../datasets/p12/processed_data/PTdict_list.npy', allow_pickle=True)\n",
    "arr_outcomes = np.load('../datasets/p12/processed_data/arr_outcomes.npy', allow_pickle=True)\n",
    "\n",
    "# remove blacklist patients\n",
    "blacklist = [140501, 150649, 140936, 143656, 141264, 145611, 142998, 147514, 142731, 150309, 155655, 156254]\n",
    "\n",
    "i = 0\n",
    "n = len(PTdict_list)\n",
    "while i < n:\n",
    "    pid = PTdict_list[i]['id']\n",
    "    if pid in blacklist:\n",
    "        PTdict_list = np.delete(PTdict_list, i)\n",
    "        arr_outcomes = np.delete(arr_outcomes, i, axis=0)\n",
    "        n -= 1\n",
    "    i += 1\n",
    "\n",
    "print(len(PTdict_list), arr_outcomes.shape)\n",
    "\n",
    "np.save('../datasets/p12/processed_data/PTdict_list.npy', PTdict_list)\n",
    "np.save('../datasets/p12/processed_data/arr_outcomes.npy', arr_outcomes)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. generate split"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle as pkl\n",
    "from torch_geometric.seed import seed_everything\n",
    "\n",
    "\n",
    "\"\"\"Use 8:1:1 split\"\"\"\n",
    "p_train = 0.80\n",
    "p_val = 0.10\n",
    "p_test = 0.10\n",
    "\n",
    "n = len(PTdict_list)  # original 12000 patients, remove 12 outliers\n",
    "n_train = round(n*p_train)\n",
    "n_val = round(n*p_val)\n",
    "n_test = n - (n_train+n_val)\n",
    "Nsplits = 5\n",
    "\n",
    "for seed in range(10):\n",
    "    seed_everything(seed)\n",
    "    for split in range(Nsplits):\n",
    "        p = np.random.permutation(n)\n",
    "        idx_train = p[:n_train]\n",
    "        idx_val = p[n_train:n_train+n_val]\n",
    "        idx_test = p[n_train+n_val:]\n",
    "        with open(f'../datasets/p12/splits/seed{seed}_split{split}.pkl', 'wb') as wbfile:\n",
    "            print(f'seed{seed}_split{split}', len(idx_train), len(idx_val), len(idx_test))\n",
    "            pkl.dump((idx_train, idx_val, idx_test), wbfile)\n",
    "\n",
    "print('split IDs saved')\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn210",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
